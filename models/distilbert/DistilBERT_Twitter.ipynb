{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef2921c8199762e",
   "metadata": {},
   "source": [
    "# DistilBERT - Training on TweetEval (Ran on Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d1d6757cd8fe5",
   "metadata": {},
   "source": [
    "## Install requirements and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9921821a12a934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:22:32.123747Z",
     "start_time": "2025-04-24T04:22:30.867077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.2.5)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.6.1)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.51.3)\r\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (3.5.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.13.2)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.30.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (19.0.1)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.11.18)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n!pip install \"transformers[torch]\"\\n!pip install \"accelerate>=0.26.0\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn torch transformers datasets\n",
    "## If running locally - uncomment and run the below package install requirements.\n",
    "\"\"\"\n",
    "!pip install \"transformers[torch]\"\n",
    "!pip install \"accelerate>=0.26.0\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc21f2d57cb2b12f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:22:38.128382Z",
     "start_time": "2025-04-24T04:22:34.744475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanshritbakshi/PycharmProjects/CS6120_final/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0425b417e5cc12",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3b268af0c60a49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:22:41.627900Z",
     "start_time": "2025-04-24T04:22:39.089515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "label_names = dataset[\"train\"].features[\"label\"].names\n",
    "num_labels  = len(label_names)\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4615102cee07df2",
   "metadata": {},
   "source": [
    "## Tokenizer and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba4c2aa2a4c2e8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:24:28.700850Z",
     "start_time": "2025-04-24T04:24:27.937598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 33543.90 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 48029.59 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 47689.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess(examples):\n",
    "    \"\"\"\n",
    "    Tokenize a batch of text examples and attach their labels for PyTorch training.\n",
    "\n",
    "    Args:\n",
    "        examples (dict):\n",
    "            A batch from a HuggingFace Dataset containing:\n",
    "              - \"text\": List[str], raw input strings to be classified.\n",
    "              - \"label\": List[int], integer class labels corresponding to each text.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            A dict with the following keys:\n",
    "              - \"input_ids\": List[List[int]], token IDs for each input, padded/truncated to max_length.\n",
    "              - \"attention_mask\": List[List[int]], mask (1 for real tokens, 0 for padding).\n",
    "              - \"labels\": List[int], the original integer labels.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    tokens[\"labels\"] = examples[\"label\"]\n",
    "    return tokens\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True)\n",
    "\n",
    "tokenized.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "train_ds = tokenized[\"train\"]\n",
    "val_ds   = tokenized[\"validation\"]\n",
    "test_ds  = tokenized[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e7a0d9a9decbc",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b9a2a407ded8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:29:49.339142Z",
     "start_time": "2025-04-24T04:25:13.897545Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/xk/v43z00vn3mj3lb_2ryh_5wxm0000gn/T/ipykernel_21564/2832962973.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 04:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>0.188821</td>\n",
       "      <td>0.930500</td>\n",
       "      <td>0.904665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanshritbakshi/PycharmProjects/CS6120_final/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: {'eval_loss': 0.18882125616073608, 'eval_accuracy': 0.9305, 'eval_f1_macro': 0.9046647877134344, 'eval_runtime': 8.244, 'eval_samples_per_second': 242.601, 'eval_steps_per_second': 7.642, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanshritbakshi/PycharmProjects/CS6120_final/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics      : {'test_loss': 0.19353729486465454, 'test_accuracy': 0.924, 'test_f1_macro': 0.8860765428569849, 'test_runtime': 8.3926, 'test_samples_per_second': 238.304, 'test_steps_per_second': 7.507}\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for multi-label classification.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple):\n",
    "            A tuple of (logits, labels)\n",
    "            - logits: np.ndarray of shape (batch_size, num_labels)\n",
    "              Raw outputs from the model’s classification head.\n",
    "            - labels: np.ndarray of shape (batch_size, num_labels)\n",
    "              Ground-truth multi-hot vectors (0/1).\n",
    "        threshold (float, optional):\n",
    "            Probability cutoff for deciding positive labels after sigmoid.\n",
    "            Defaults to 0.3.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            {\n",
    "                \"f1_micro\": float,\n",
    "                    The micro-averaged F1 score across all labels.\n",
    "                \"subset_accuracy\": float,\n",
    "                    The fraction of samples where the predicted multi-hot\n",
    "                    vector exactly matches the ground truth.\n",
    "            }\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "#TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_emotion\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=9,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics= compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Metrics:\", trainer.evaluate())\n",
    "print(\"Test Metrics      :\", trainer.predict(test_ds).metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9ed4e0211dd64",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41fb785415312e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:30:07.316648Z",
     "start_time": "2025-04-24T04:29:53.006481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9240\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.96      0.96      0.96       581\n",
      "         joy       0.96      0.93      0.94       695\n",
      "        love       0.78      0.88      0.83       159\n",
      "       anger       0.92      0.93      0.93       275\n",
      "        fear       0.91      0.88      0.90       224\n",
      "    surprise       0.76      0.77      0.77        66\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.88      0.89      0.89      2000\n",
      "weighted avg       0.93      0.92      0.92      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>1394</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>1275</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1801</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>1704</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>1756</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>1918</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TN  FP  FN   TP\n",
       "Class                      \n",
       "sadness   1394  25  25  556\n",
       "joy       1275  30  48  647\n",
       "love      1801  40  19  140\n",
       "anger     1704  21  19  256\n",
       "fear      1756  20  26  198\n",
       "surprise  1918  16  15   51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "ds = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "y_true, y_pred = [], []\n",
    "for ex in ds:\n",
    "    text = ex[\"text\"]\n",
    "    true = ex[\"label\"]\n",
    "    enc  = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "\n",
    "    # single‐label prediction via softmax\n",
    "    probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    pred  = int(np.argmax(probs))\n",
    "\n",
    "    y_true.append(true)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "#Overall accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Overall Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "# 5) Per‐class precision / recall / F1\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=label_names,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#Unfold into TP, FP, FN, TN per class\n",
    "rows = []\n",
    "total = cm.sum()\n",
    "for idx, label in enumerate(label_names):\n",
    "    TP = cm[idx, idx]\n",
    "    FP = cm[:, idx].sum() - TP\n",
    "    FN = cm[idx, :].sum() - TP\n",
    "    TN = total - (TP + FP + FN)\n",
    "    rows.append({\n",
    "        \"Class\": label,\n",
    "        \"TN\": TN,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TP\": TP,\n",
    "    })\n",
    "\n",
    "#Display\n",
    "df_conf_stats = pd.DataFrame(rows).set_index(\"Class\")\n",
    "display(df_conf_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1181694b2f16c",
   "metadata": {},
   "source": [
    "## Testing on Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9614b9369bfe28c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:41:27.143870Z",
     "start_time": "2025-04-24T04:41:26.996173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> I just got my dream job—feeling on top of the world!\n",
      "→ joy (98.0%)\n",
      "\n",
      "> Why does everything always go wrong for me?\n",
      "→ anger (55.8%)\n",
      "\n",
      "> That plot twist in the movie made me jump!\n",
      "→ joy (35.3%)\n",
      "\n",
      "> I can not wait to go home\n",
      "→ sadness (42.6%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def predict_emotion_local(text: str):\n",
    "    \"\"\"\n",
    "    Predict the most likely emotion for a given piece of text.\n",
    "\n",
    "    This function tokenizes the input string, passes it through the\n",
    "    fine-tuned BERT model, applies a softmax to obtain class probabilities,\n",
    "    and returns the label with the highest probability along with its confidence score.\n",
    "\n",
    "    Args:\n",
    "        text (str):\n",
    "            A single input string to classify.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, float]:\n",
    "            - emotion: The name of the predicted emotion label (from `label_names`).\n",
    "            - confidence: The softmax probability of the predicted label, in [0.0, 1.0].\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits          # shape (1, num_labels)\n",
    "        probs  = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    best_id    = int(probs.argmax())\n",
    "    emotion    = label_names[best_id]\n",
    "    confidence = probs[best_id]\n",
    "\n",
    "    return emotion, confidence\n",
    "\n",
    "for txt in [\n",
    "    \"I just got my dream job—feeling on top of the world!\",\n",
    "    \"Why does everything always go wrong for me?\",\n",
    "    \"That plot twist in the movie made me jump!\",\n",
    "    \"I can not wait to go home\"\n",
    "]:\n",
    "    emo, conf = predict_emotion_local(txt)\n",
    "    print(f\"> {txt}\\n→ {emo} ({conf:.1%})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
