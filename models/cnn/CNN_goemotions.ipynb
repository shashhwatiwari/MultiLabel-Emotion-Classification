{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "    concatenate, Dropout, Dense\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bb188",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73283995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GoEmotions\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "val_texts   = dataset[\"validation\"][\"text\"]\n",
    "test_texts  = dataset[\"test\"][\"text\"]\n",
    "train_labels = dataset[\"train\"][\"labels\"]\n",
    "val_labels   = dataset[\"validation\"][\"labels\"]\n",
    "test_labels  = dataset[\"test\"][\"labels\"]\n",
    "label_names  = dataset[\"train\"].features[\"labels\"].feature.names  # 28 emotion labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63717674",
   "metadata": {},
   "source": [
    "### Tokenization and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf5437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi‑hot encode labels\n",
    "num_labels = len(label_names)\n",
    "def to_multi_hot(label_lists):\n",
    "    m = np.zeros((len(label_lists), num_labels), dtype=np.int32)\n",
    "    for i, labs in enumerate(label_lists):\n",
    "        m[i, labs] = 1\n",
    "    return m\n",
    "\n",
    "y_train = to_multi_hot(train_labels)\n",
    "y_val   = to_multi_hot(val_labels)\n",
    "y_test  = to_multi_hot(test_labels)\n",
    "\n",
    "# Tokenize and pad\n",
    "vocab_size = 20000  \n",
    "max_len    = 100     \n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f008fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer_goemotions.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer saved to tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3da936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    seq = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "X_train = encode(train_texts)\n",
    "X_val   = encode(val_texts)\n",
    "X_test  = encode(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cbc4ec",
   "metadata": {},
   "source": [
    "### Model architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model architecture\n",
    "embedding_dim = 128       \n",
    "filter_sizes  = [3,4,5]   \n",
    "num_filters   = 128       \n",
    "drop_rate     = 0.5       \n",
    "\n",
    "inputs = Input(shape=(max_len,), dtype=\"int32\")\n",
    "embed  = Embedding(vocab_size, embedding_dim, input_length=max_len)(inputs)\n",
    "\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Conv1D(filters=num_filters, kernel_size=sz, activation=\"relu\")(embed)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    conv_blocks.append(pool)\n",
    "\n",
    "concat = concatenate(conv_blocks)\n",
    "drop   = Dropout(drop_rate)(concat)\n",
    "output = Dense(num_labels, activation=\"sigmoid\")(drop)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "\n",
    "# Compile & train\n",
    "learning_rate = 1e-3\n",
    "batch_size    = 64\n",
    "epochs        = 8\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4aec0",
   "metadata": {},
   "source": [
    "### Evaluation and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffbc78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 1s 11ms/step - loss: 0.1049 - accuracy: 0.5229\n",
      "Test Loss: 0.1049, Test Accuracy: 0.5229\n",
      "85/85 [==============================] - 1s 10ms/step\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1049 - accuracy: 0.5229\n",
      "\n",
      "Test Loss: 0.1049\n",
      "Keras Accuracy (element‑wise): 0.5229\n",
      "Raw Accuracy (sklearn, element‑wise): 0.9662\n",
      "Micro-averaged F1 score: 0.5123\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration     0.7023    0.5337    0.6065       504\n",
      "     amusement     0.7834    0.6439    0.7069       264\n",
      "         anger     0.5844    0.2273    0.3273       198\n",
      "     annoyance     0.4912    0.0875    0.1485       320\n",
      "      approval     0.5893    0.0940    0.1622       351\n",
      "        caring     0.5333    0.1185    0.1939       135\n",
      "     confusion     0.6316    0.1569    0.2513       153\n",
      "     curiosity     0.4643    0.1373    0.2120       284\n",
      "        desire     0.5385    0.1687    0.2569        83\n",
      "disappointment     0.7500    0.0596    0.1104       151\n",
      "   disapproval     0.4500    0.0674    0.1173       267\n",
      "       disgust     0.6279    0.2195    0.3253       123\n",
      " embarrassment     0.6364    0.1892    0.2917        37\n",
      "    excitement     0.7059    0.2330    0.3504       103\n",
      "          fear     0.7708    0.4744    0.5873        78\n",
      "     gratitude     0.9251    0.9119    0.9185       352\n",
      "         grief     0.0000    0.0000    0.0000         6\n",
      "           joy     0.6500    0.4037    0.4981       161\n",
      "          love     0.7778    0.7059    0.7401       238\n",
      "   nervousness     0.0000    0.0000    0.0000        23\n",
      "      optimism     0.7412    0.3387    0.4649       186\n",
      "         pride     0.0000    0.0000    0.0000        16\n",
      "   realization     0.7143    0.0690    0.1258       145\n",
      "        relief     0.0000    0.0000    0.0000        11\n",
      "       remorse     0.5781    0.6607    0.6167        56\n",
      "       sadness     0.6588    0.3590    0.4647       156\n",
      "      surprise     0.5714    0.3404    0.4267       141\n",
      "       neutral     0.5721    0.6525    0.6097      1787\n",
      "\n",
      "     micro avg     0.6431    0.4257    0.5123      6329\n",
      "     macro avg     0.5517    0.2805    0.3397      6329\n",
      "  weighted avg     0.6237    0.4257    0.4629      6329\n",
      "   samples avg     0.4722    0.4525    0.4542      6329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "pred_probs = model.predict(X_test, batch_size=batch_size)\n",
    "y_pred     = (pred_probs >= 0.5).astype(int)\n",
    "\n",
    "# Keras evaluation (loss + element‑wise accuracy)\n",
    "loss, keras_acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "print(f\"Keras Accuracy (element‑wise): {keras_acc:.4f}\")\n",
    "\n",
    "# Raw accuracy (fraction of individual label predictions correct)\n",
    "raw_acc = accuracy_score(\n",
    "    y_test.flatten(),\n",
    "    y_pred.flatten()\n",
    ")\n",
    "print(f\"Raw Accuracy (sklearn, element‑wise): {raw_acc:.4f}\")\n",
    "\n",
    "# Micro‑averaged F1 (treats every label equally across all samples)\n",
    "micro_f1 = f1_score(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    average='micro',\n",
    "    zero_division=0\n",
    ")\n",
    "print(f\"Micro-averaged F1 score: {micro_f1:.4f}\\n\")\n",
    "\n",
    "# Full per‑label classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=label_names,\n",
    "    zero_division=0,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aca200",
   "metadata": {},
   "source": [
    "### Code cell for interactive testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66aa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction function\n",
    "def predict_emotions(text, top_k=5, threshold=0.2):\n",
    "    seq    = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    preds  = model.predict(padded)[0]\n",
    "    # pair labels with scores and sort\n",
    "    pairs = sorted(zip(label_names, preds), key=lambda x: x[1], reverse=True)\n",
    "    return [(label, float(score)) for label, score in pairs if score >= threshold][:top_k]\n",
    "\n",
    "def interactive_predict_emotions():\n",
    "    print(\"\\nEnter a sentence to classify emotions (type 'quit' to exit):\")\n",
    "    while True:\n",
    "        user_input = input(\"> \")\n",
    "        if user_input.lower() in (\"quit\", \"exit\"):\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        results = predict_emotions(user_input)\n",
    "        if results:\n",
    "            print(\"Predicted emotions:\")\n",
    "            for label, score in results:\n",
    "                print(f\"  {label:>10s}: {score:.3f}\")\n",
    "        else:\n",
    "            print(\"No emotion score exceeded the threshold. Try a different sentence.\")\n",
    "\n",
    "#interactive_predict_emotions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb82a4",
   "metadata": {},
   "source": [
    "### Code cell to export and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb785047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_and_zip(model,\n",
    "                   model_filename='cnn_goemotions.h5',\n",
    "                   zip_filename='cnn_goemotions.zip'):\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "#export_and_zip(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
