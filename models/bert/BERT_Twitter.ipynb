{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BERT - Training on TweetEval (Ran on Colab)",
   "id": "9ef2921c8199762e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install requirements and import packages",
   "id": "ea3d1d6757cd8fe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:00:14.383448Z",
     "start_time": "2025-04-24T04:00:13.174342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install pandas numpy scikit-learn torch transformers datasets\n",
    "## If running locally - uncomment and run the below package install requirements.\n",
    "\"\"\"\n",
    "!pip install \"transformers[torch]\"\n",
    "!pip install \"accelerate>=0.26.0\"\n",
    "\"\"\""
   ],
   "id": "2a9921821a12a934",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.2.5)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.6.1)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.51.3)\r\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (3.5.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.13.2)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2024.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.30.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (19.0.1)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.11.18)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n!pip install \"transformers[torch]\"\\n!pip install \"accelerate>=0.26.0\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:00:40.964395Z",
     "start_time": "2025-04-24T04:00:37.726119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix"
   ],
   "id": "fc21f2d57cb2b12f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanshritbakshi/PycharmProjects/CS6120_final/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Dataset",
   "id": "db0425b417e5cc12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:01:06.960400Z",
     "start_time": "2025-04-24T04:01:06.187673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "label_names = dataset[\"train\"].features[\"label\"].names\n",
    "num_labels  = len(label_names)\n",
    "print(label_names)"
   ],
   "id": "bb3b268af0c60a49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer and Data Preprocessing",
   "id": "b4615102cee07df2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:04:39.930263Z",
     "start_time": "2025-04-24T04:04:38.721261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess(examples):\n",
    "    \"\"\"\n",
    "    Tokenize a batch of text examples and attach their labels for PyTorch training.\n",
    "\n",
    "    Args:\n",
    "        examples (dict):\n",
    "            A batch from a HuggingFace Dataset containing:\n",
    "              - \"text\": List[str], raw input strings to be classified.\n",
    "              - \"label\": List[int], integer class labels corresponding to each text.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            A dict with the following keys:\n",
    "              - \"input_ids\": List[List[int]], token IDs for each input, padded/truncated to max_length.\n",
    "              - \"attention_mask\": List[List[int]], mask (1 for real tokens, 0 for padding).\n",
    "              - \"labels\": List[int], the original integer labels.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    tokens[\"labels\"] = examples[\"label\"]\n",
    "    return tokens\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True)\n",
    "\n",
    "tokenized.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "train_ds = tokenized[\"train\"]\n",
    "val_ds   = tokenized[\"validation\"]\n",
    "test_ds  = tokenized[\"test\"]"
   ],
   "id": "6ba4c2aa2a4c2e8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 27427.64 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 35817.99 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 36068.40 examples/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the model",
   "id": "a22e7a0d9a9decbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:14:37.655864Z",
     "start_time": "2025-04-24T04:05:42.612287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for multi-label classification.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple):\n",
    "            A tuple of (logits, labels)\n",
    "            - logits: np.ndarray of shape (batch_size, num_labels)\n",
    "              Raw outputs from the model’s classification head.\n",
    "            - labels: np.ndarray of shape (batch_size, num_labels)\n",
    "              Ground-truth multi-hot vectors (0/1).\n",
    "        threshold (float, optional):\n",
    "            Probability cutoff for deciding positive labels after sigmoid.\n",
    "            Defaults to 0.3.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            {\n",
    "                \"f1_micro\": float,\n",
    "                    The micro-averaged F1 score across all labels.\n",
    "                \"subset_accuracy\": float,\n",
    "                    The fraction of samples where the predicted multi-hot\n",
    "                    vector exactly matches the ground truth.\n",
    "            }\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_emotion\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics= compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#  Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "print(\"Validation Metrics:\", trainer.evaluate())\n",
    "print(\"Test Metrics      :\", trainer.predict(test_ds).metrics)"
   ],
   "id": "bb3b9a2a407ded8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/xk/v43z00vn3mj3lb_2ryh_5wxm0000gn/T/ipykernel_20587/3961963973.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/sanshritbakshi/PycharmProjects/CS6120_final/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 08:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.168896</td>\n",
       "      <td>0.929500</td>\n",
       "      <td>0.904892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanshritbakshi/PycharmProjects/CS6120_final/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: {'eval_loss': 0.16889630258083344, 'eval_accuracy': 0.9295, 'eval_f1_macro': 0.9048923431164505, 'eval_runtime': 15.6894, 'eval_samples_per_second': 127.474, 'eval_steps_per_second': 4.015, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanshritbakshi/PycharmProjects/CS6120_final/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics      : {'test_loss': 0.19651179015636444, 'test_accuracy': 0.9215, 'test_f1_macro': 0.8830407173564044, 'test_runtime': 15.7298, 'test_samples_per_second': 127.147, 'test_steps_per_second': 4.005}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Results",
   "id": "22f9ed4e0211dd64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:20:18.640875Z",
     "start_time": "2025-04-24T04:19:55.836365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "ds = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "y_true, y_pred = [], []\n",
    "for ex in ds:\n",
    "    text = ex[\"text\"]\n",
    "    true = ex[\"label\"]\n",
    "    enc  = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "\n",
    "    # single‐label prediction via softmax\n",
    "    probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    pred  = int(np.argmax(probs))\n",
    "\n",
    "    y_true.append(true)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "#Overall accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Overall Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "#Per‐class precision / recall / F1\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=label_names,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#  Unfold into TP, FP, FN, TN per class\n",
    "rows = []\n",
    "total = cm.sum()\n",
    "for idx, label in enumerate(label_names):\n",
    "    TP = cm[idx, idx]\n",
    "    FP = cm[:, idx].sum() - TP\n",
    "    FN = cm[idx, :].sum() - TP\n",
    "    TN = total - (TP + FP + FN)\n",
    "    rows.append({\n",
    "        \"Class\": label,\n",
    "        \"TN\": TN,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TP\": TP,\n",
    "    })\n",
    "\n",
    "# Display\n",
    "df_conf_stats = pd.DataFrame(rows).set_index(\"Class\")\n",
    "display(df_conf_stats)"
   ],
   "id": "b41fb785415312e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9215\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.96      0.96      0.96       581\n",
      "         joy       0.96      0.93      0.94       695\n",
      "        love       0.78      0.89      0.83       159\n",
      "       anger       0.92      0.90      0.91       275\n",
      "        fear       0.88      0.89      0.89       224\n",
      "    surprise       0.72      0.82      0.77        66\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.87      0.90      0.88      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            TN  FP  FN   TP\n",
       "Class                      \n",
       "sadness   1397  22  26  555\n",
       "joy       1278  27  50  645\n",
       "love      1801  40  18  141\n",
       "anger     1704  21  27  248\n",
       "fear      1750  26  24  200\n",
       "surprise  1913  21  12   54"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>1397</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>1278</td>\n",
       "      <td>27</td>\n",
       "      <td>50</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1801</td>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>1704</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>1750</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>1913</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing on Text",
   "id": "b4495af43e080b2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:43:44.855143Z",
     "start_time": "2025-04-24T04:43:40.762453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "def predict_emotion_local(text: str):\n",
    "    \"\"\"\n",
    "    Predict the most likely emotion for a given piece of text.\n",
    "\n",
    "    This function tokenizes the input string, passes it through the\n",
    "    fine-tuned BERT model, applies a softmax to obtain class probabilities,\n",
    "    and returns the label with the highest probability along with its confidence score.\n",
    "\n",
    "    Args:\n",
    "        text (str):\n",
    "            A single input string to classify.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, float]:\n",
    "            - emotion: The name of the predicted emotion label (from `label_names`).\n",
    "            - confidence: The softmax probability of the predicted label, in [0.0, 1.0].\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits          # shape (1, num_labels)\n",
    "        probs  = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    best_id    = int(probs.argmax())\n",
    "    emotion    = label_names[best_id]\n",
    "    confidence = probs[best_id]\n",
    "\n",
    "    return emotion, confidence\n",
    "\n",
    "for txt in [\n",
    "    \"I just got my dream job—feeling on top of the world!\",\n",
    "    \"Why does everything always go wrong for me?\",\n",
    "    \"That plot twist in the movie made me jump!\",\n",
    "    \"I can not wait to go home\"\n",
    "]:\n",
    "    emo, conf = predict_emotion_local(txt)\n",
    "    print(f\"> {txt}\\n→ {emo} ({conf:.1%})\\n\")"
   ],
   "id": "9dca46a7a904d0ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> I just got my dream job—feeling on top of the world!\n",
      "→ joy (99.3%)\n",
      "\n",
      "> Why does everything always go wrong for me?\n",
      "→ anger (79.2%)\n",
      "\n",
      "> That plot twist in the movie made me jump!\n",
      "→ surprise (59.1%)\n",
      "\n",
      "> I can not wait to go home\n",
      "→ joy (65.8%)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1c80e386dded4d17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
